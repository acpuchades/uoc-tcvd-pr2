---
title: "Resolución PR2"
authors:
  - Alejandro Caravaca Puchades
  - Andrea Vigo Ruíz
format:
  html:
    toc: true
    df-print: paged
    number-sections: true
---

```{r}
library(ggplot2)
library(missRanger)
library(naniar)
library(stopwords)
library(tidytext)
library(tidyverse)
library(wordcloud2)
```

```{r}
datos <- read_csv("data/idealista-sale-properties-spain.csv")
```

```{r}
dim(datos)
```

```{r}
n_distinct(datos)
```

# Descripción del dataset

**¿Por qué es importante y qué pregunta/problema pretende responder? Resume brevemente las variables que lo forman y su tamaño.**

```{r}
head(datos)
```

```{r}
str(datos)
```

```{r}
#| cached: true
features_words <- datos |>
  unnest_tokens(word, features) |>
  filter(!word %in% c(0:9,stopwords("es"))) |>
  count(word, sort = TRUE)

head(features_words)
```

```{r}
#| cached: true
description_words <- datos |>
  unnest_tokens(word, description) |>
  filter(!word %in% c(0:9,stopwords("es"))) |>
  count(word, sort = TRUE)

wordcloud2(description_words)
```

# Integración y selección de los datos de interés a analizar

**Puede ser el resultado de adicionar diferentes datasets o una subselección útil de los datos originales, en base al objetivo que se quiera conseguir. Si se decide trabajar con una selección de los datos, es muy importante que esta esté debidamente justificada. Además, se recomienda mostrar un resumen de los datos que permita ver a simple vista las diferentes variables y sus rangos de valores.**

Como hemos visto, en el conjunto de datos encontramos una serie de campos de texto libre. Aunque este tipo de campos permiten una gran flexibilidad a la hora de almacenar información, añaden cierta complejidad al análisis. A continuación, extraeremos una serie de campos estructurados a partir de la información que hemos obtenido del análisis de texto anterior:

```{r}
datos_ext <- datos |>
  mutate(
    calle = str_extract(address, regex("^[^,]+(,\\s*([0-9]+|s/n)\\s*[^,]*)*", ignore_case=TRUE)),
    area_region = str_remove(address, regex("^[^,]+(,\\s*([0-9]+|s/n)\\s*[^,]*)*,", ignore_case=TRUE)),
  ) |>
  separate_wider_delim(
    area_region,
    delim = ",",
    names=c("area", "ciudad", "provincia"),
    too_many = "merge",
    too_few = "align_end",
  ) |>
  mutate(
    across(c(calle, area, ciudad, provincia), ~ .x |>
             str_trim() |> str_to_upper() |>
             stringi::stri_trans_general("Latin-ASCII")),
    superficie = features |>
      str_to_lower() |>
      str_extract("([0-9]+) m²", group=1),
    habitaciones = features |>
      str_to_lower() |>
      str_extract("([0-9]+) hab.", group=1),
    piso = case_when(
      features |> str_to_lower() |> str_detect("bajo") ~ "B",
      features |> str_to_lower() |> str_detect("entreplanta") ~ "E",
      TRUE ~ features |> str_to_lower() |> str_extract("planta ([0-9]+)[ºª]", group=1),
    ),
    ascensor = features |>
      str_to_lower() |>
      str_extract("(con|sin) ascensor", group=1) |>
      case_match("con" ~ TRUE, "sin" ~ FALSE, .default=FALSE),
    exterior = features |>
      str_to_lower() |>
      str_extract("(exterior|interior)", group=1) |>
      case_match("exterior" ~ TRUE, "interior" ~ FALSE, .default=FALSE),
    garaje = features |> str_to_lower() |> str_detect("garaje"),
  )
```

# Limpieza de los datos

**¿Los datos contienen ceros, elementos vacíos u otros valores numéricos que indiquen la pérdida de datos? Gestiona cada uno de estos casos utilizando el método de imputación que consideres más adecuado.**

**Identifica y gestiona adecuadamente el tipo de dato de cada atributo (p.ej. conversión de variables categóricas en factor).**

**Identifica y gestiona los valores extremos.**

**Justifica la necesidad de otros métodos de limpieza para este dataset en particular y, de ser necesario, aplícalos.**

> **Nota**: se ha decidido contestar los apartados anteriores en un orden diferente al que aparecen en el enunciado con el objetivo de facilitar la tarea y su comprensión. En este caso, se ha decidido primero transformar los datos a un formato más adecuado para su análisis y después aplicar las técnicas de limpieza e imputación de datos.

Tal y como veíamos en primer lugar, el dataset contiene una serie de variables almacenadas en formato de cadena y que deberían transformarse en variables numéricas o factores, previo a analizar sus correspondientes rangos y niveles, así como valores extremos:

```{r}
datos_conv <- datos_ext |>
  mutate(
    property_id = as.character(property_id),
    price = price |> str_remove("\\.") |> as.numeric(),
    price_m2 = price_m2 |> str_remove("\\.") |> str_remove(" €/m²") |> as.numeric(),
    across(c(habitaciones, superficie), as.numeric),
    across(c(area, ciudad, provincia, piso), ~ .x |> str_to_upper() |> as.factor()),
    across(c(ascensor, exterior, garaje), ~if_else(.x, "S", "N") |> factor(levels = c("N", "S"))),
  )
```

Vamos en primer lugar a hacer un análisis preliminar de las características de los datos, atendiendo especialmente al rango de datos en las variables numéricas:

```{r}
summary(datos_conv |> select(where(is.numeric)))
```

Llama la atención el valor máximo de las variables _energy_consumption_ y _energy_emissions_, en particular su valor máximo (9999) se aleja mucho de la media y la mediana correspondientes. Vamos a comprobar la existencia de valores extremos:

```{r}
datos_conv |>
  select(where(is.numeric)) |>
  pivot_longer(everything()) |>
  ggplot(aes(x = value)) +
    geom_histogram(bins=20) +
    facet_wrap(~name, scales="free")
```

De los histogramas anteriores parece desprenderse que la mayoría de las variables numéricas muestran una distribución con una marcada desviación hacia la derecha. Llama la atención la existencia de entradas con valor cero para las variables _price_ (`r sum(datos_conv$price==0)` entradas), _price_m2_ (`r sum(datos_conv$price_m2 == 0)` entradas) y _superficie_ (`r sum(datos_conv$superficie==0)` entradas).

```{r}
datos_conv <- mutate(datos_conv, across(c(price, price_m2, superficie), ~ na_if(.x, 0)))
```

```{r}
vis_miss(datos_conv)
```

Queda claro que la mayoría de los valores perdidos pertenecen a las variables _energy_consumption_ y _energy_emissions_, seguidos de las variables _piso_, _distrito_ y _ciudad_.

```{r}
#| cache: true
datos_imp <- missRanger(
  datos_conv, data_only = TRUE,
  formula = price + superficie + energy_consumption + energy_emissions +
            piso + ascensor + garaje + exterior + habitaciones +
            area + ciudad + provincia ~
    . - title - description - features - property_id - property_url
      - address - agent_ref - last_updated - scraped_date,
  num.trees = 50, max.depth = 5, pmm.k = 5, seed = 123,
)
```

```{r}
vis_miss(datos_imp)
```

# Análisis de los datos

```{python}
import numpy as np
import pandas as pd
import sklearn as sk
import seaborn as sns
import matplotlib.pyplot as plt
```

```{python}
from rpy2 import robjects
from rpy2.robjects import r, pandas2ri

pandas2ri.activate()
datos = pandas2ri.rpy2py(r['datos_imp'])
datos.head()
```

```{python}
from sklearn.model_selection import train_test_split

datos_X = datos[[
  "superficie", "habitaciones", "piso", "ascensor",
  "exterior", "garaje", "area",  "ciudad", "provincia"
]]

datos_y = datos["price"]

X_train, X_test, y_train, y_test = train_test_split(
    datos_X, datos_y, test_size=0.2, random_state=1234
)
```

**Aplica un modelo supervisado y uno no supervisado a los datos y comenta los resultados obtenidos.**

```{python}
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

numeric_features = ['superficie', 'habitaciones']
categorical_features = [
  'piso', 'ascensor', 'exterior',
  'garaje', 'area', 'ciudad', 'provincia'
]

preprocessor = make_column_transformer(
  (StandardScaler(), numeric_features),
  (OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
)
```

```{python}
#| cache: true

from sklearn.pipeline import make_pipeline
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.metrics import root_mean_squared_error, r2_score

gbr_model = make_pipeline(
  preprocessor,
  HistGradientBoostingRegressor(max_iter=1000, learning_rate=0.1,
                                max_depth=10, random_state=123),
)

gbr_model.fit(X_train, y_train)
y_pred = gbr_model.predict(X_test)

print("RMSE:", root_mean_squared_error(y_test, y_pred))
print("R²:", r2_score(y_test, y_pred))
```

```{python}
fig, ax = plt.subplots()
ax.scatter(x=y_pred, y=y_test, s=3, alpha=0.2)
ax.set_xlabel("Predicción del precio")
ax.set_ylabel("Precio real")
plt.show()
```

```{python}
#| cache: true

from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import root_mean_squared_error, r2_score

rf_model = make_pipeline(
  preprocessor,
  RandomForestRegressor(n_estimators=500, random_state=123),
)

rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)

print("RMSE:", root_mean_squared_error(y_test, y_pred))
print("R²:", r2_score(y_test, y_pred))
```

**Aplica una prueba por contraste de hipótesis. Ten en cuenta que algunas de estas pruebas requieren verificar previamente la normalidad y homocedasticidad de los datos.**

```{r}
ggplot(datos_conv, aes(ascensor, price_m2)) + geom_boxplot() + scale_y_log10()
```

```{r}
hist(datos_conv$price_m2)
```

```{r}
qqnorm(datos_conv$price_m2)
qqline(datos_conv$price_m2)
```

```{r}
hist(sqrt(datos_conv$price_m2))
```

```{python}
from statsmodels.stats.diagnostic import kstest_normal

res = kstest_normal(datos.price_m2)
print(f"Normality test for property prices: {res[1]:.3f}")
```

```{python}
import statsmodels.stats.nonparametric as nonp

lift_properties = datos[datos.ascensor == 2]
nolift_properties = datos[datos.ascensor == 1]
res = nonp.rank_compare_2indep(lift_properties.price_m2, nolift_properties.price_m2)
print(f"Mann-Whitney's U rank-sum test: W={res.statistic:.3f}, p-value={res.pvalue:.3f}")
```

```{r}
hist(datos_conv$price_m2 ^ (1/4))
```

```{r}
qqnorm(datos_conv$price_m2 ^ (1/4))
qqline(datos_conv$price_m2 ^ (1/4))
```

# Representación de los resultados a partir de tablas y gráficas

**Este apartado se puede responder a lo largo de la práctica, sin necesidad de concentrar todas las representaciones en este apartado. Se debe representar tanto el contenido del dataset para observar las proporciones y distribuciones de las diferentes variables una vez aplicada la etapa de limpieza, como los resultados obtenidos tras la etapa de análisis.**

# Resolución del problema.

**A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?**
