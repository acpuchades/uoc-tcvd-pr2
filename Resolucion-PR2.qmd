---
title: "Resolución PR2"
authors:
  - Alejandro Caravaca Puchades
  - Andrea Vigo Ruíz
format:
  html:
    toc: true
    toc-depth: 2
    df-print: paged
    number-sections: true
    embed-resources: true
---

```{r}
library(ggplot2)
library(missRanger)
library(naniar)
library(stopwords)
library(tidytext)
library(tidyverse)
library(wordcloud2)
```

```{r}
datos <- read_csv("data/idealista-sale-properties-spain.csv")
```

```{r}
dim(datos)
```

```{r}
n_distinct(datos)
```

# Descripción del dataset

**¿Por qué es importante y qué pregunta/problema pretende responder? Resume brevemente las variables que lo forman y su tamaño.**

Asumiremos que somos una startup con la intención de ofrecer un servicio de búsqueda de propiedades inmobiliarias en España. Para ello, hemos recopilado un conjunto de datos de propiedades en venta a través del portal Idealista, que contiene información sobre las características de las propiedades, su ubicación y otros detalles relevantes.

Nuestra intención es doble: por un lado, pretendemos poder ofrecer asesoramiento personalizado a nuestros clientes sobre el valor real de los inmuebles que se encuentran ofertados, por lo que sería interesante disponer de un modelo que nos permita estimar el precio de un inmueble en función de su ubicación y características.

Por el otro lado, puesto que nos encontramos en una fase inicial del negocio, nos gustaría conocer si existen distintos perfiles de propiedades que nos permitan segmentar el mercado. De esta manera, podremos contratar a asesoradores especializados en distintos tipos de propiedad para ofrecer un trato personalizado.

Para resolver estas cuestiones, vamos a utilizar el conjunto de datos que describimos en la práctica anterior, el cual consiste en un conjunto de datos con `r nrow(datos)` entradas y `r ncol(datos)` columnas. Cada entrada corresponde a una propiedad inmobiliaria en venta en la página web de Idealista, y las columnas contienen información sobre las características de la propiedad, su ubicación, el precio, entre otros.

```{r}
head(datos)
```

```{r}
str(datos)
```

```{r}
#| cached: true
features_words <- datos |>
  unnest_tokens(word, features) |>
  filter(!word %in% c(0:9,stopwords("es"))) |>
  count(word, sort = TRUE)

head(features_words)
```

```{r}
#| cached: true
description_words <- datos |>
  unnest_tokens(word, description) |>
  filter(!word %in% c(0:9,stopwords("es"))) |>
  count(word, sort = TRUE)

wordcloud2(description_words)
```

# Integración y selección de los datos de interés a analizar

**Puede ser el resultado de adicionar diferentes datasets o una subselección útil de los datos originales, en base al objetivo que se quiera conseguir. Si se decide trabajar con una selección de los datos, es muy importante que esta esté debidamente justificada. Además, se recomienda mostrar un resumen de los datos que permita ver a simple vista las diferentes variables y sus rangos de valores.**

Como hemos visto, en el conjunto de datos encontramos una serie de campos de texto libre. Aunque este tipo de campos permiten una gran flexibilidad a la hora de almacenar información, añaden cierta complejidad al análisis. A continuación, extraeremos una serie de campos estructurados a partir de la información que hemos obtenido del análisis de texto anterior:

```{r}
normalize_address <- function(x) {
  x |>
    str_to_upper() |>
    stringi::stri_trans_general("Latin-ASCII") |>
    str_replace_all("C/", "CALLE ") |>
    str_replace_all("CARRER", "CALLE ") |>
    str_replace_all("AV/", "AVENIDA ") |>
    str_replace_all("AV.", "AVENIDA ") |>
    str_replace_all("AVINGUDA", "AVENIDA ") |>
    str_replace_all("\\s+", " ") |>
    str_replace_all(",+", ",") |>
    str_trim()
}

extract_calle <- function(x) {
  norm_address <- normalize_address(x)
  norm_address |>
    str_extract("^([^,0-9]+)") |>
    str_replace("S/N", "") |>
    str_trim()
}

extract_provincia <- function(x) {
  address_parts <- normalize_address(x) |> str_split(",")
  map_chr(address_parts, function(parts) {
    if (length(parts) >= 2) {
      str_trim(parts[length(parts)])
    } else {
      NA_character_
    }
  })
}

extract_ciudad <- function(x) {
  address_parts <- normalize_address(x) |> str_split(",")
  map_chr(address_parts, function(parts) {
    if (length(parts) >= 3) {
      str_trim(parts[length(parts) - 1])
    } else {
      NA_character_
    }
  })
}

extract_distrito <- function(x) {
  norm_address <- normalize_address(x)
  address_parts <- str_split(norm_address, ",")
  map_chr(seq_along(norm_address), function(i) {
    addr <- norm_address[i]
    parts <- address_parts[[i]]
    if (str_detect(addr, "DISTRITO")) {
      str_extract(addr, "DISTRITO [^,]+")
    } else if (length(parts) >= 4) {
      str_trim(parts[length(parts) - 2])
    } else {
      NA_character_
    }
  })
}

extract_barrio <- function(x) {
  norm_address <- normalize_address(x)
  address_parts <- str_split(norm_address, ",")
  map_chr(seq_along(norm_address), function(i) {
    addr <- norm_address[i]
    parts <- address_parts[[i]]
    if (str_detect(addr, "BARRIO")) {
      str_extract(addr, "BARRIO [^,]+")
    } else if (length(parts) >= 5) {
      str_trim(parts[length(parts) - 3])
    } else {
      NA_character_
    }
  })
}

datos_ext <- datos |>
  mutate(
    calle = extract_calle(address),
    barrio = extract_barrio(address),
    distrito = extract_distrito(address),
    ciudad = extract_ciudad(address),
    provincia = extract_provincia(address),
    superficie = features |>
      str_to_lower() |>
      str_extract("([0-9]+) m²", group=1),
    habitaciones = features |>
      str_to_lower() |>
      str_extract("([0-9]+) hab.", group=1),
    piso = case_when(
      features |> str_to_lower() |> str_detect("bajo") ~ "B",
      features |> str_to_lower() |> str_detect("entreplanta") ~ "E",
      TRUE ~ features |> str_to_lower() |> str_extract("planta ([0-9]+)[ºª]", group=1),
    ),
    ascensor = features |>
      str_to_lower() |>
      str_extract("(con|sin) ascensor", group=1) |>
      case_match("con" ~ TRUE, "sin" ~ FALSE, .default=FALSE),
    exterior = features |>
      str_to_lower() |>
      str_extract("(exterior|interior)", group=1) |>
      case_match("exterior" ~ TRUE, "interior" ~ FALSE, .default=FALSE),
    garaje = features |> str_to_lower() |> str_detect("garaje"),
  )
```

# Limpieza de los datos

**¿Los datos contienen ceros, elementos vacíos u otros valores numéricos que indiquen la pérdida de datos? Gestiona cada uno de estos casos utilizando el método de imputación que consideres más adecuado.**

**Identifica y gestiona adecuadamente el tipo de dato de cada atributo (p.ej. conversión de variables categóricas en factor).**

**Identifica y gestiona los valores extremos.**

**Justifica la necesidad de otros métodos de limpieza para este dataset en particular y, de ser necesario, aplícalos.**

> **Nota**: se ha decidido contestar los apartados anteriores en un orden diferente al que aparecen en el enunciado con el objetivo de facilitar la tarea y su comprensión. En este caso, se ha decidido primero transformar los datos a un formato más adecuado para su análisis y después aplicar las técnicas de limpieza e imputación de datos.

Tal y como veíamos en primer lugar, el dataset contiene una serie de variables almacenadas en formato de cadena y que deberían transformarse en variables numéricas o factores, previo a analizar sus correspondientes rangos y niveles, así como valores extremos:

```{r}
datos_conv <- datos_ext |>
  mutate(
    property_id = as.character(property_id),
    price = price |> str_remove("\\.") |> as.numeric(),
    price_m2 = price_m2 |> str_remove("\\.") |> str_remove(" €/m²") |> as.numeric(),
    across(c(habitaciones, superficie), as.numeric),
    across(c(calle, barrio, distrito, ciudad, provincia, piso),
           ~ .x |> str_to_upper() |> as.factor()),
    across(c(ascensor, exterior, garaje), ~if_else(.x, "S", "N") |> factor(levels = c("N", "S"))),
  )
```

Vamos en primer lugar a hacer un análisis preliminar de las características de los datos, atendiendo especialmente al rango de datos en las variables numéricas:

```{r}
summary(datos_conv |> select(where(is.numeric)))
```

Llama la atención el valor máximo de las variables _energy_consumption_ y _energy_emissions_, en particular su valor máximo (9999) se aleja mucho de la media y la mediana correspondientes. Vamos a comprobar la existencia de valores extremos:

```{r}
datos_conv |>
  select(where(is.numeric)) |>
  pivot_longer(everything()) |>
  ggplot(aes(x = value)) +
    geom_histogram(bins=20) +
    facet_wrap(~name, scales="free")
```

De los histogramas anteriores parece desprenderse que la mayoría de las variables numéricas muestran una distribución con una marcada desviación hacia la derecha. Llama la atención la existencia de entradas con valor cero para las variables _price_ (`r sum(datos_conv$price==0)` entradas), _price_m2_ (`r sum(datos_conv$price_m2 == 0)` entradas) y _superficie_ (`r sum(datos_conv$superficie==0)` entradas).

```{r}
datos_conv <- mutate(datos_conv, across(c(price, price_m2, superficie), ~ na_if(.x, 0)))
```

```{r}
vis_miss(datos_conv)
```

Queda claro que la mayoría de los valores perdidos pertenecen a las variables _energy_consumption_ y _energy_emissions_, seguidos de las variables _piso_, _distrito_ y _ciudad_.

```{r}
#| cache: true
datos_imp <- missRanger(
  datos_conv, data_only = TRUE,
  formula = superficie + energy_consumption + energy_emissions +
            piso + ascensor + garaje + exterior + habitaciones +
            calle + barrio + distrito + ciudad + provincia ~
    . - title - description - features - property_id - property_url
      - address - agent_ref - last_updated - scraped_date,
  num.trees = 50, max.depth = 5, pmm.k = 5, seed = 123,
)
```

```{r}
vis_miss(datos_imp)
```

Una vez hemos realizado la imputación de las variables predictoras, comprobemos ahora la distribución de las variable a predecir:

```{r}
hist(datos_imp$price)
```

Encontramos que la distribución es muy sesgada hacia la derecha, por lo que vamos a aplicar una transformación de la variable para reducir su asimetría. En este caso, vamos a aplicar una transformación de la familia Box-Cox, que es adecuada para variables con distribuciones sesgadas. Tal y como veremos posteriormente, la transformación necesaria en nuestro caso es la transformación Box-Cox con $\lambda=\dfrac{1}{4}$:

```{r}
hist(datos_imp$price ^ (1/4))
```

Encontamos que la variable precio incluso tras ser transformada, parece seguir una distribución bimodal, lo cual probablemente afectará el rendimiento de nuestro modelo. Veamos si podemos superar este escollo utilizando la variable price_m2 en su lugar:

```{r}
hist(datos_imp$price_m2^(1/4))
```

Observamos que la distribución de la variable de price_m2 es mucho más simétrica, por lo que la utilizaremos como variable a predecir en lugar de la variable price. Si el precio del inmueble es lo que nos interesa, podremos posteriormente multiplicar la variable price_m2 predicha por la superficie de cada inmueble.

# Análisis de los datos

```{python}
import numpy as np
import pandas as pd
import sklearn as sk
import seaborn as sns
import matplotlib.pyplot as plt
```

```{python}
from rpy2 import robjects
from rpy2.robjects import r, pandas2ri, Environment

pandas2ri.activate()
datos = pandas2ri.rpy2py(r['datos_imp'])
datos = datos[~datos.price_m2.isna()]
datos.head()
```

```{python}
from sklearn.model_selection import train_test_split

datos_X = datos[[
  "superficie", "habitaciones", "piso", "ascensor", "exterior", "garaje",
  "calle", "barrio", "distrito", "ciudad", "provincia"
]]

datos_y = datos["price_m2"]

X_train, X_test, y_train, y_test = train_test_split(
    datos_X, datos_y, test_size=0.2, random_state=1234
)
```

**Aplica un modelo supervisado y uno no supervisado a los datos y comenta los resultados obtenidos.**

En primer lugar, desarrollaremos un modelo de regresión con el objetivo de predecir el precio de mercado en función de las características del inmueble. Dada la naturaleza tabular de nuestros datos, probaremos dos modelos diferentes que suelen presentar un buen desempeño en este tipo de conjuntos de datos: un modelo de regresión basado en árboles de decisión y otro basado en _gradient boosting_.

```{python}
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

numeric_features = ['superficie', 'habitaciones']
categorical_features = [
  'piso', 'ascensor', 'exterior', 'garaje',
  'calle', 'barrio', 'distrito', 'ciudad', 'provincia'
]

def boxcox_transform(lam):
    return lambda x: (np.power(x, lam) - 1) / lam

def inv_boxcox_transform(lam):
    return lambda y: np.power((lam * y) + 1, 1 / lam)

preprocessor = make_column_transformer(
  (StandardScaler(), numeric_features),
  (OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
)
```

```{python}
#| cache: true

from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeRegressor
from sklearn.compose import TransformedTargetRegressor
from sklearn.metrics import root_mean_squared_error, r2_score

dt_model = make_pipeline(
  preprocessor,
  DecisionTreeRegressor(max_depth=10, random_state=123),
)

dt_model_t = TransformedTargetRegressor(
    regressor=dt_model,
    func=boxcox_transform(1/4),
    inverse_func=inv_boxcox_transform(1/4)
)

dt_model_t.fit(X_train, y_train)
y_pred = dt_model_t.predict(X_test)

print("RMSE:", root_mean_squared_error(y_test, y_pred))
print("R²:", r2_score(y_test, y_pred))
```

```{python}
#| cache: true

from sklearn.pipeline import make_pipeline
from sklearn.compose import TransformedTargetRegressor
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.metrics import root_mean_squared_error, r2_score

gbr_model = make_pipeline(
  preprocessor,
  HistGradientBoostingRegressor(max_iter=1000, learning_rate=0.1,
                                max_depth=10, random_state=123),
)

gbr_model_t = TransformedTargetRegressor(
    regressor=gbr_model,
    func=boxcox_transform(1/4),
    inverse_func=inv_boxcox_transform(1/4)
)

gbr_model_t.fit(X_train, y_train)
y_pred = gbr_model_t.predict(X_test)

print("RMSE:", root_mean_squared_error(y_test, y_pred))
print("R²:", r2_score(y_test, y_pred))
```

```{python}
plt.figure(figsize=(6, 4))
plt.scatter(x=y_pred, y=y_test, s=3, alpha=0.2)
plt.axline((0, 0), (1, 1), color='gray', linestyle='--')
plt.xlabel("Predicción del precio/m2")
plt.ylabel("Precio/m2 real")
plt.show()
```

```{python}
y_resid = y_test - y_pred

plt.figure(figsize=(6, 4))
sns.scatterplot(x=y_pred, y=y_resid, s=3, alpha=0.5)
plt.axhline(0, color='gray', linestyle='--')
plt.xlabel("Predicted price/m²")
plt.ylabel("Residual")
plt.title("Residuals vs. Predicted")
plt.tight_layout()
plt.show()
```

**Aplica una prueba por contraste de hipótesis. Ten en cuenta que algunas de estas pruebas requieren verificar previamente la normalidad y homocedasticidad de los datos.**

Durante una de nuestras numerosas y tediosas reuniones internas, uno de nuestros asesores inmobiliarios mencionó que, en su experiencia, las propiedades con ascensor tienden a tener un precio por metro cuadrado más alto que aquellas sin ascensor. Otro de nuestros asesores, sin embargo, argumentó que esta diferencia no es significativa y que el precio por metro cuadrado depende más de la ubicación y otras características de la propiedad.

Puesto que nos caracterizamos por ser una organización orientada al dato, pretendemos resolver esta cuestión de forma objetiva, utilizando una prueba estadística que nos permita determinar si existe una diferencia significativa en el precio por metro cuadrado entre las propiedades con y sin ascensor.

```{r}
ggplot(datos_conv, aes(ascensor, price_m2)) + geom_boxplot() + scale_y_log10()
```

Vamos en primer lugar a observar la distribución de los precios de las propiedades en nuestro conjunto de datos:

```{r}
hist(datos_conv$price_m2)
```

```{r}
qqnorm(datos_conv$price_m2)
qqline(datos_conv$price_m2)
```

```{python}
from statsmodels.stats.diagnostic import kstest_normal

res = kstest_normal(datos.price_m2)
print(f"Normality test for property prices: {res[1]:.3f}")
```

> **Nota**: el test de normalidad de Kolmogorov-Smirnov indica que los precios por metro cuadrado no siguen una distribución normal, con un p-valor muy bajo (p < 0.001). No obstante, dado el elevado número de registros presentes en el conjunto de datos, este test puede ser muy sensible a pequeñas desviaciones de la normalidad. Por ello, es necesario matizar que nos apoyamos fundamentalmente en el análisis Q-Q.

Es evidente que los precios no parecen seguir una distribución normal, por lo que a priori no podemos aplicar una prueba paramétrica como la t de Student. Se decide por tanto aplicar una prueba no paramétrica, como la prueba U de Mann-Whitney, que es adecuada para comparar dos muestras independientes cuando no se cumplen los supuestos de normalidad de los datos:

```{python}
import statsmodels.stats.nonparametric as nonp

lift_properties = datos[datos.ascensor == 2]
nolift_properties = datos[datos.ascensor == 1]
res = nonp.rank_compare_2indep(lift_properties.price_m2, nolift_properties.price_m2)
print(f"Mann-Whitney's U rank-sum test: W={res.statistic:.3f}, p-value={res.pvalue:.3f}")
```

Comprobamos que el resultado de la prueba U de Mann-Whitney es significativo, lo que indica que existe una diferencia en el precio por metro cuadrado entre las propiedades con y sin ascensor. Podríamos aquí concluir nuestro análisis al respecto. Sin embargo, vamos a comprobar si podemos aplicar una normalización de los datos que nos permita aplicar una prueba paramétrica, como la t de Student, para comparar las medias de los dos grupos.

Dada la marcada desviación de los datos hacia la derecha, vamos a aplicar una serie de transformaciones de la familia Box-Cox, que son adecuadas para transformar variables con distribuciones sesgadas. Probaremos inicialmente con la transformación cuadrática, que es una transformación comúnmente utilizada para reducir la asimetría de los datos ($\lambda=\dfrac{1}{2}$):

```{r}
hist(sqrt(datos_conv$price_m2))
```

Vemos que no resulta suficiente. Tras varias pruebas de ensayo y error, encontramos que la transformación con $\lambda=\dfrac{1}{4}$ es la que muestra una distribución de los datos más similar a la normalidad:

```{r}
hist(datos_conv$price_m2 ^ (1/4))
```

```{r}
qqnorm(datos_conv$price_m2 ^ (1/4))
qqline(datos_conv$price_m2 ^ (1/4))
```

Una vez aplicada la transformación, comprobamos que los datos parecen seguir una distribución normal, por lo que podemos aplicar una prueba paramétrica como la t de Student. Antes de ello, vamos a comprobar si las varianzas de los dos grupos son homogéneas, lo cual es un supuesto necesario para aplicar la prueba t:

```{python}
from scipy.stats import levene

res = levene(
    lift_properties.price_m2 ** (1/4),
    nolift_properties.price_m2 ** (1/4),
    center='mean'
)

print(f"Levene's test for homogeneity of variances: F={res.statistic:.3f}, p-value={res.pvalue:.3f}")
```

Dado que la prueba de Levene indica que las varianzas de los dos grupos son heterogéneas (p < 0.01), aplicaremos la versión de Welch de la prueba t, que no asume homogeneidad de varianzas:

```{python}
from statsmodels.stats.weightstats import ttest_ind

res = ttest_ind(
    lift_properties.price_m2 ** (1/4),
    nolift_properties.price_m2 ** (1/4),
    usevar='unequal'
)

print(f"Welch's t-test: t={res[0]:.3f}, p-value={res[1]:.3f}")
```

Como se puede comprobar, la conclusión derivada de ambos análisis es la misma: las propiedades con ascensor tienen en general un precio por metro cuadrado más alto que aquellas sin ascensor, y esta diferencia es estadísticamente significativa. Por lo tanto, podemos concluir que la afirmación de nuestro asesor inmobiliario es correcta.

# Representación de los resultados a partir de tablas y gráficas

**Este apartado se puede responder a lo largo de la práctica, sin necesidad de concentrar todas las representaciones en este apartado. Se debe representar tanto el contenido del dataset para observar las proporciones y distribuciones de las diferentes variables una vez aplicada la etapa de limpieza, como los resultados obtenidos tras la etapa de análisis.**

# Resolución del problema.

**A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?**
